# General Solution（整体方案：数据→最优解→训练→强化学习微调）

本文档给出一条“端到端”的可复现路线：从真实数据抓取开始，构建小规模可精确求解的数据集，再训练一个可解释的神经策略网络，并用强化学习对目标函数进行微调。

> 一句话总结：
> 我们用确定性搜索/DP 解决“小规模最优解”，再用神经网络学习“可扩展的近似最优策略”，并用 RL 直接对齐原始 objective。

---

## 1. 真实数据获取：爬虫抓取 WNBA 球员特征池

### 1.1 数据来源与目标

- 来源：Basketball-Reference 的 WNBA 年度 advanced 页面（按赛季）。
- 目标：为“候选球员池”提供尽可能真实的统计特征分布（能力向量、出场等）。

### 1.2 处理流程

1) 下载 HTML（每赛季一页）
2) 解析表格字段 → 统一字段命名/缺失值处理
3) 形成“球员特征池”（pool），作为后续实例生成的采样来源

---

## 2. 数据增强：从真实分布生成可控的小规模实例

传统精确算法只能在小规模下稳定运行；但纯合成数据容易与真实分布脱节。因此我们采用“真实+合成混合”的数据增强方案。

### 2.1 为什么要增强

- 真实赛季的球队决策样本太少（且不可直接观测到最优策略序列）。
- 纯合成会导致能力/薪资的边际分布不真实，影响模型泛化。
- 我们需要大量可标注的训练对：`instance → DP-opt solution`。

### 2.2 增强策略（核心思想）

给定一个“球员池”，为每条样本随机生成一个多赛季实例：

- 采样候选球员集合（例如 `n_players=15`）
- 为每个赛季生成球员能力矩阵与薪资（可做轻微扰动/跨年漂移）
- 生成环境参数（工资帽、赛季场次、对手强度等）
- 固定规模参数（如 `T=3, K=6, L=11, U=12`）确保可精确求解

并支持混合比例（real-frac）：

- 一部分样本尽量贴近真实 pool 的原始分布
- 另一部分引入扰动与组合，增加覆盖面与鲁棒性

---

## 3. 确定性搜索 + 动态规划：为小规模实例生成全局最优解

### 3.1 动作空间压缩：枚举可行 roster mask

直接把阵容选择视为 $2^{n}$ 的子集选择不可行。

我们做法是：

- 先枚举所有满足人数约束的 roster（bitmask）
- 再过滤工资帽约束，得到每赛季的“可行动作集合”

这样每个时间步的动作从指数空间变为“有限离散集合”（例如 `n=15, L=11, U=12` 时约 1820 个）。

### 3.2 DP 结构

在多赛季（短规划期）下，状态可以用“上一赛季阵容 + 当季环境/球员特征”刻画。

DP 的典型形式是：

$$
V_t(s_t) = \max_{a_t \in \mathcal{A}(s_t)} \left[ r(s_t,a_t) + \gamma V_{t+1}(s_{t+1}) \right]
$$

其中转移包含：

- 新阵容对下一赛季的“延续/流失（churn）”影响
- 目标函数中胜场与利润项的权衡（见 [model_general.md](model_general.md)）

最终得到：

- 每条实例的最优 objective $J^*(s)$
- 每个赛季的最优动作序列（最优 roster mask 序列）

并导出为监督训练数据对。

---

## 4. 基于小规模最优解训练策略网络（BC：模仿学习）

### 4.1 监督信号是什么

对每条训练样本，我们有 DP 输出的最优序列：

- 输入：`instance`（多赛季球员特征 + 环境 + 初始阵容）
- 标签：`solution.masks[t]`（每赛季最优动作 mask）

### 4.2 训练目标

把“动作选择”变成一个分类问题：

- 模型输出对所有可行 mask 的 logits
- 用交叉熵让最优 mask 的概率最大

BC 的作用：

- 快速学到“像 DP 一样”的初始策略
- 为后续 RL 提供稳定的初始化（避免从随机策略开始探索）

---

## 5. 强化学习微调：直接优化原始 objective（Actor-Critic + AWBC）

BC 只能模仿特定数据集上的 DP 行为；在分布偏移、以及“模型表达能力/解码策略”变化时，纯模仿会卡在一个 plateau。

因此我们用 RL 在原始 reward 上微调。

### 5.1 环境如何给出 reward

每个 episode 对应一条多赛季实例：

- 状态 $s_t$：由当季环境、球员特征、上季阵容等组成
- 动作 $a_t$：选择一个可行 roster mask
- reward $r_t$：由题目的 objective 分解得到（胜场/利润/流失惩罚等）

### 5.2 Actor-Critic 的梯度从哪里来

- Actor 学习提升“高回报动作”的概率（policy gradient）
- Critic 学习估计 $V(s_t)$ 作为 baseline，降低方差
- 最终通过 $\nabla_\theta \log \pi_\theta(a_t|s_t)$ 把 reward 信号传播回网络参数

### 5.3 为什么还要 AWBC

RL 会引入探索噪声，容易偏离原先“可行且合理”的 DP 策略。

AWBC（优势加权模仿）的直觉：

- 当 rollout 的优势 $A_t$ 为正时，说明这条行为比当前价值估计更好
- 这时既增强该行为的概率，也保持对老师动作的偏好（但强度随优势调整）

---

## 6. 最终交付物（可复现结果）

本仓库的工程侧会产出训练权重、曲线、评估缓存等文件，这些属于“可复现实验产物”，通常不会随论文/报告一同提交（已加入 `.gitignore`，你可以本地留作备份）。

原则上你只需要记住：

- 训练产物输出目录由训练命令的 `--out-dir` 决定
- 评估产物输出目录由评估命令的 `--out-dir` 决定
- 如需把图片放到报告里引用，使用导出命令把图片复制到一个稳定的文档目录（见 README 与 plots.md）

更多细节：

- 训练与 RL 细节、网络可解释性说明：见 [train_strategy.md](train_strategy.md)
- 指标曲线图如何阅读：见 [plots.md](plots.md)
