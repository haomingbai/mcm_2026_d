@misc{lim2020tft,
  title={Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting},
  author={Bryan Lim and Sercan O. Arik and Nicolas Loeff and Tomas Pfister},
  year={2020},
  eprint={1912.09363},
  archivePrefix={arXiv},
  primaryClass={stat.ML},
  url={https://arxiv.org/abs/1912.09363}
}

@misc{gfg_branch_bound,
  title={Introduction to Branch and Bound - Data Structures and Algorithms Tutorial},
  author={{GeeksforGeeks}},
  year={2025},
  note={Last accessed 2~Feb~2026},
  url={https://www.geeksforgeeks.org/dsa/introduction-to-branch-and-bound-data-structures-and-algorithms-tutorial/}
}

@misc{darvariu2024graph_rl,
  title={Graph Reinforcement Learning for Combinatorial Optimization: A Survey and Unifying Perspective},
  author={Victor-Alexandru Darvariu and Stephen Hailes and Mirco Musolesi},
  year={2024},
  eprint={2404.06492},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2404.06492}
}

@misc{herhoopstats_salary,
  title={{WNBA CBA and Salary Cap Explained}},
  author={{Her Hoop Stats}},
  year={2025},
  note={Salary cap table listing each season's cap and minimum; retrieved 2~Feb~2026},
  url={https://herhoopstats.com/wnba_cba_salary_cap_explained}
}

@misc{herhoopstats_roster,
  title={{WNBA CBA and Salary Cap Explained}},
  author={{Her Hoop Stats}},
  year={2025},
  note={Section on roster size states teams must carry 11--12 players; retrieved 2~Feb~2026},
  url={https://herhoopstats.com/wnba_cba_salary_cap_explained}
}

@article{perez2019_players_selection,
  title={Players’ selection for basketball teams, through Performance Index Rating, using multiobjective evolutionary algorithms},
  author={Miguel {\'A}ngel P{\'e}rez-Toledano and Francisco J. Rodriguez and Javier Garc{\'i}a-Rubio and Sergio Jos{\'e} Iba{\~n}ez},
  journal={PLOS ONE},
  year={2019},
  volume={14},
  number={9},
  pages={e0221258},
  doi={10.1371/journal.pone.0221258},
  note={Discusses multiobjective team selection; notes that exact determination of the Pareto front is infeasible for large instances},
  url={https://doi.org/10.1371/journal.pone.0221258}
}

@article{nbc2025_pay_us,
  title={Why WNBA players are telling the league: ‘Pay us what you owe us’},
  author={Rohan Nadkarni},
  journal={NBC Sports Bay Area},
  year={2025},
  note={Reports that WNBA revenue increased from \$102\,million to \$119\,million between 2019 and 2023 and that players receive only 9.3\% of league revenue; highlights expansion to 18 teams and new media rights deal},
  url={https://www.nbcsportsbayarea.com/wnba/wnba-revenue-sharing-cba-salaries/1863230/}
}

@article{bonema2025_revenue_sharing,
  title={Rethinking Revenue Sharing in the WNBA},
  author={Mary Bonema},
  journal={Brooklyn Sports \& Entertainment Law Blog},
  year={2025},
  note={Discusses that WNBA viewership is at an all-time high yet player salaries represent the smallest share of league revenue among U.S. sports; cites that players receive about 9.3\% of revenue and describes CBA negotiations amid a new \$2.2\,billion media rights deal},
  url={https://sports-entertainment.brooklaw.edu/sports/rethinking-revenue-sharing-in-the-wnba/}
}

@inproceedings{radke2023_multiagent,
  title={Presenting Multiagent Challenges in Team Sports Analytics},
  author={David Radke and Alexi Orchard},
  booktitle={Proceedings of the 22nd International Conference on Autonomous Agents and Multiagent Systems (AAMAS)},
  year={2023},
  pages={1--5},
  note={Argues that invasion games like basketball provide rich environments for multi-agent systems research; emphasises challenges of coordination across short-term tactics and long-term management},
  url={https://arxiv.org/abs/2303.13660}
}

@article{bhattacharjee2024_fantasy_rl,
  title={Optimizing Fantasy Sports Team Selection with Deep Reinforcement Learning},
  author={Shamik Bhattacharjee and Kamlesh Marathe and Nilesh Patil and Hitesh Kapoor},
  journal={Proceedings of the CODS-COMAD Conference},
  year={2024},
  note={Frames team creation as a sequential decision problem and shows that reinforcement learning algorithms can construct competitive fantasy teams by maximising expected performance},
  url={https://arxiv.org/abs/2412.19215}
}

@article{voepel2025_expansion,
  title={WNBA team expansion FAQ: Cleveland, Detroit, Philadelphia},
  author={Michael Voepel and Kevin Pelton and Kendra Andrews},
  journal={ESPN},
  year={2025},
  note={Reports that the WNBA will expand to 18 teams by 2030 and discusses the implications for the CBA and player revenue share},
  url={https://www.espn.com/wnba/story/_/id/45618874/wnba-expansion-cleveland-detroit-philadelphia-cba-draft}
}

@article{fieldhouse2025_breakout,
  title={The WNBA's Breakout Year: A 2024 Season of Records, Revenue, and the Caitlin Clark Effect},
  author={Scott Agness},
  journal={Fieldhouse Files},
  year={2025},
  note={Highlights that the 2024 WNBA season set attendance and viewership records, with total attendance up 48\% and average attendance at 9{,}807 fans per game},
  url={https://www.fieldhousefiles.com/p/2024-wnba-breakout-season-caitlin-clark}
}

@misc{copland2025_kaggle_stats,
  title={WNBA Player and Team Stats 2003–2025 (120k rows)},
  author={Nicholas Copland},
  year={2025},
  note={Dataset with over 120{,}000 rows of WNBA player and team statistics from 2003 through 2025; includes box scores compiled from the wehoop.sportsdataverse.org library. Accessed 2~Feb~2026},
  url={https://www.kaggle.com/datasets/nicholascoplandunc/wnba-player-and-team-stats-2003-2025-120k-rows}
}

@misc{gfg_lstm,
  title={What is LSTM - Long Short Term Memory?},
  author={{GeeksforGeeks}},
  year={2025},
  note={Introduces long short–term memory networks, describing their memory cell and three gating mechanisms (input, forget and output gates) and explaining how these gates regulate information flow and mitigate vanishing gradients},
  url={https://www.geeksforgeeks.org/deep-learning/deep-learning-introduction-to-long-short-term-memory/}
}

@misc{gfg_transformer,
  title={Transformers in Machine Learning},
  author={{GeeksforGeeks}},
  year={2025},
  note={Provides an overview of the Transformer architecture, covering self–attention, multi–head attention, positional encodings, feed–forward layers and the benefits over recurrent models},
  url={https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/}
}